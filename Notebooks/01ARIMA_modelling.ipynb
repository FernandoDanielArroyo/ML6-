{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA forecasting üìà\n",
    "\n",
    "Objectives:\n",
    "* Walk-through the steps involved in time series forecasting\n",
    "* Introduce ARIMA models\n",
    "* Use an arima model to make a forecast\n",
    "* Excercise: Automatize the analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import what you need here\n",
    "\n",
    "#!pip install pmdarima and import it\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pmdarima "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example 1 - airline passenger forecasts ‚úàÔ∏è üí∫\n",
    "\n",
    "Our goal here is to forecast the number of air-travel passengers (per month) over the next 12 months using historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the airline passenger data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "from pmdarima.datasets import load_airpassengers\n",
    "\n",
    "START_DATE = '1949-01-01'\n",
    "airline = load_airpassengers(as_series=True)\n",
    "\n",
    "#there's no datetimeindex from the bundled dataset. So let's add one.\n",
    "airline.index= pd.date_range(START_DATE, periods=len(airline), freq='MS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than a random train test split, when analysing time series data we must use the most recent data as the **testing set**.\n",
    "\n",
    "TODO: Write a function which splits a dataset (a dataframe with time as index) and returns a train and test datasets, this function will have the split date as an argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your function and code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### check train and test sizes are what we expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore TRAIN data\n",
    "\n",
    "Like any prediction process we explore the data set to gain insights. But (as always) we only plot the TRAIN data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different months have different numbers of days. Soetimes we need to do what we can to make things easier for our model. Calcuating number of passengers per day might help remove some variation from the series..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore subcomponents in series.\n",
    "\n",
    "Before performing a forecast it is worth decomposing the time series into its components of trend, seasonality and noise.\n",
    "\n",
    "* statsmodels has function called `seasonal_decompose()` for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive forecasting\n",
    "\n",
    "We need a baseline (as always!) to know if our more complex models are any use! We explore two here:\n",
    "* seasonal naive - a model which just takes the same period from the previous season. ( in this case the same month last year)\n",
    "* naive - just carry forward the last value in the series\n",
    "\n",
    "Given the strong seasonal component that was confirmed by the seasonal decomposition it may be a good idea to use a **seasonal naive** forecasting method.  This is part of the 'carry forward previous values' family of *naive* forecasting methods.  In general, if we have data with period $k$ are at time $t$ and we are predicting time $Y_{t+1}$ then we simply carry forward the value from $Y_{t+1-k}$. In other words we have yearly data so we just take the value from the same month last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what this is doing lets plot its predictions for the whole TRAIN data set to which it has been fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the **residuals** can give us information about how the model is performing and the errors it is making.  Sometimes there are reffered to as **in sample** diagnostics. This just means we are looking at diagnostics of data which has been used to fit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluating error\n",
    "\n",
    "In the past we have done this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For timeseries there are other approaches...\n",
    "\n",
    "RMSE and MAE are called 'scale dependent' measures as the units and magnitude are specific to the problem and context.  An alternative approach is to use a scale invariant measure such as the **mean absolute percentage error (MAPE)**\n",
    "\n",
    "The percentage error is given by $p_t = 100e_t/y_t$ where $e_t$ is the error in predicting $y_t$.  Therefore, MAPE = $mean(|p_t|)$. A limitation of MAPE is that it is inflated when the denominator is small relative to the absolute forecast error (such in the case of outliers or extreme unexpected events). It is also penalises negative errors more than positive errors.  A consequence of this property is that MAPE can lead to selecting a model that tends to under forecast.  The two following examples illustrate the issue. $$APE_{1} = \\left| \\frac{y_t - \\hat{y_t}}{y_t} \\right|= \\left| \\frac{150 - 100}{150} \\right| = \\frac{50}{150} = 33.33\\%$$  \n",
    "\n",
    "$$APE_{2} = \\left| \\frac{100 - 150}{100} \\right| = \\frac{50}{100} = 50\\%$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    '''\n",
    "    MAPE\n",
    "\n",
    "    Parameters:\n",
    "    --------\n",
    "    y_true -- np.array actual observations from time series\n",
    "    y_pred -- the predictions to evaluate\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float, scalar value representing the MAPE (0-100)\n",
    "    '''\n",
    "    # code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Perform the same analysis for the naive prediction\n",
    "\n",
    "* Have a go at using the `Naive1()` class.  It follows the same pattern as SNaive interface.  Instantiate a class.  call the `.fit(y_train)` method and then use the `.fittedvalues` and `.resid` properties for diagnostics.  \n",
    "* Calculate the in-sample RMSE and MAPE\n",
    "* What happens to the insample residuals if you fit the raw training data to the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The predictions we made above are only for the next step (month) in each timeseries! (not for the next 12 months) This means they are not correctly evaluating the error that we need for the specific task at hand. You can imagine for the simple naive model that a 12 month prediction actually looks like a flat line (using the most recent value). We will see this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA\n",
    "\n",
    "Let's try an ARIMA model at last. Using a classical statsitical approach you would select the order of model which produces a model with acceptable residual plots (remember the first part of linear regression module?). Selecting the best model can also be done automatically using packages which do it for you! Wohoo! üéâ\n",
    "\n",
    "The `pmdarima` package is an excellent forecasting library for building ARIMA models.  I recommend it over and above the options available in core `statsmodels` package.  It is easier to use and offers an `auto_arima()` function that iteratively searches for a model that minimises the **Akaike Information Criterion (AIC)**\n",
    "\n",
    "* ${\\displaystyle \\mathrm {AIC} \\,=\\,2k-2\\ln({\\hat {L}})}$\n",
    "\n",
    "where $k$ = number of parameters in the model and $\\hat{L}$ is the maximum value of the likelihood function for the model.  A likelihood function measures the 'goodness' of fit of a model to data given a set of parameters.  \n",
    "\n",
    "This looks very complicated at first, but all we need to remember that the models we are working with are very flexible. This means that we can easily create complex models that overfit. Recall that overfitting is when a model will predict the training data exceptionally well, but will perform poorly on out of sample data.  The form of AIC means that it rewards models that fit the training data well, but also penalises models with high $k$ (complicated models with lots of parameters).  That means that AIC will prefer simpler models - in turn reducing overfitting.  That's a great formaula for automatically selecting a good ARIMA forecasting model.\n",
    "\n",
    "There's a large amount of theory about how to build an ARIMA model.  But modern applications tend to opt for the auto approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA models require data to be stationary. Stationarity includes that both the mean and variance do not change over time. ARIMA models can take into account an increasing mean over time, however not the variance. We therefore need another transformation. A log transformation will take care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#select a model that minimises AIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model selected is of order (2, 0, 0)x(0, 1, 1, 12) (and the residuals look acceptable). We will use these model in cross validation to estimate our model performance.\n",
    "\n",
    "We can also specify a model with specific parameters as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series cross-validation\n",
    "\n",
    "In reality we would use some sort of cross validation. However, we must be careful when using time series which approach we use. We must be careful not to give the model any information from the future that it would not otherwise have at the time of making a forecast. This means we cannot use the standard CV methods which randomise our data that we have used in the past.\n",
    "\n",
    "In the classicial time series literature time series cross validation is called a **Rolling Forecasting Horizon**. This is explained in details [here](https://robjhyndman.com/hyndsight/tscv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries CV with naive models on  airline data\n",
    "\n",
    "In order to choose between our models we use CV to estimate how we think each will perform on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is better for the seasonal-naive forecast. We would choose this as our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV with ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a far better model that either of the naive results. We would choose this model over the others. Is it suitable for use in practice?.... ca depend..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "* go back and try different step values in the RollingForecastCV\n",
    "* What changes?\n",
    "* In each case what size is the data is the model training on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions and evaluation on the TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce final forecast with chosen model\n",
    "\n",
    "We want to predict the next 12 steps after the data we currently have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1 - Automatize the analysis\n",
    "\n",
    "TODO: Write a class with the following methods:\n",
    "* The necessary arguments to split the dataset, fit the auto_arima model etc.\n",
    "* A `fit` method which fits the auto_arima naive and snaive models using a train dataset\n",
    "* A `get_metrics` method which computes the MAPE for each models using a test dataset\n",
    "* A `predict` method which takes as input a prediction horizon and returns predicted values.\n",
    "* A `plot` method which plots the data, and if available the predictions \n",
    "\n",
    "Bonus: \n",
    "* Add a method get_cv Which performs cross validation\n",
    "* To further automatize the process add an automatic detection of the period.\n",
    "* Add the possibility to apply a preprocessing of the data before the fit: such as a log transformation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Test this method on all the other [11 datasets](https://alkaline-ml.com/pmdarima/modules/classes.html#module-pmdarima.datasets) of pmdarima. To help you, we provide the list of dataset names and a function to load any of these datasets.\n",
    "\n",
    "On some of them the method does not work well, why? What could be done to improve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_4_gpu]",
   "language": "python",
   "name": "conda-env-ML_4_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "55ae904272fbc2017e639122b3e139edf7fbab9fdbc6c93acd70dfd03d08b820"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
